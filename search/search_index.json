{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Code Snippets (By Debasis Das) Here we will cover basic to advanced topics on the following Python Machine Learning Design Patterns Algorithms Data Structure Cocoa Swift","title":"Home"},{"location":"#welcome-to-code-snippets-by-debasis-das","text":"Here we will cover basic to advanced topics on the following Python Machine Learning Design Patterns Algorithms Data Structure Cocoa Swift","title":"Welcome to Code Snippets (By Debasis Das)"},{"location":"Cocoa/Cocoa/","text":"Cocoa Sample Code User Interface Elements NSTableView NSOutlineView NSCollectionView","title":"Cocoa"},{"location":"Cocoa/Cocoa/#cocoa-sample-code","text":"","title":"Cocoa Sample Code"},{"location":"Cocoa/Cocoa/#user-interface-elements","text":"NSTableView NSOutlineView NSCollectionView","title":"User Interface Elements"},{"location":"Datastructures/Datastructures/","text":"Data Structures Stack Stack is one of the simplest data structures. Stack has LIFO - Last In first out order Stack is like an array but with limited functionality Push to add a new object to the top Pop to remove the last object top to look at the top most/last element without removing it Swift - Stack Implementation Tree Tree is a simple data structure used to represent hierarchies such as Org Hierarchy Class Hierarchy Family Tree etc Swift - Tree Implementation","title":"Introduction"},{"location":"Datastructures/Datastructures/#data-structures","text":"","title":"Data Structures"},{"location":"Datastructures/Datastructures/#stack","text":"Stack is one of the simplest data structures. Stack has LIFO - Last In first out order Stack is like an array but with limited functionality Push to add a new object to the top Pop to remove the last object top to look at the top most/last element without removing it Swift - Stack Implementation","title":"Stack"},{"location":"Datastructures/Datastructures/#tree","text":"Tree is a simple data structure used to represent hierarchies such as Org Hierarchy Class Hierarchy Family Tree etc Swift - Tree Implementation","title":"Tree"},{"location":"Datastructures/Stack/","text":"Stack is one of the simplest data structures. Stack has LIFO - Last In first out order Stack is like an array but with limited functionality Push to add a new object to the top Pop to remove the last object top to look at the top most/last element without removing it import Cocoa //Here the stack is created as a wrapper around a swift array that allows push an object to the stack, //pop the last object and look at the top element of the stack public struct Stack < T > { fileprivate var array = [ T ]() public var isEmpty : Bool { return array . isEmpty } public var count : Int { return array . count } public mutating func push ( _ element : T ){ array . append ( element ) } public mutating func pop () -> T ?{ return array . popLast () } public var top : T ? { return array . last } } Testing the stack implementation var stack = Stack < Int >() stack . push ( 10 ) print ( stack ) //Stack<Int>(array: [10]) stack . push ( 20 ) print ( stack ) //Stack<Int>(array: [10, 20]) stack . push ( 30 ) print ( stack ) //Stack<Int>(array: [10, 20, 30]) if let val = stack . pop (){ print ( \"The popped item = \\( val ) \" ) //The popped item = 30 } print ( stack ) //Stack<Int>(array: [10, 20]) if let topElement = stack . top { print ( \"The Top Element is = \\( topElement ) \" ) } //The Top Element is = 20 print ( stack ) //Stack<Int>(array: [10, 20]) //The pop removes the last element however the top only looks at the last element without removing it.","title":"Stack"},{"location":"Datastructures/Tree/","text":"Tree Datastructure implementation in Swift import Cocoa class Node { var value : String = \"\" var children : [ Node ] = [] weak var parent : Node ? init ( value : String ) { self . value = value } func addChild ( child : Node ){ self . children . append ( child ) child . parent = self } func search ( value : String ) -> Node ?{ if value == self . value { return self } for child in self . children { if let foundObj = child . search ( value : value ){ return foundObj } } return nil } } extension Node : CustomStringConvertible { var description : String { var text = \" \\( value ) \" if ! children . isEmpty { text = text + \" \\n\\t Children = {\" for child in children { text = text + child . description + \",\" } text = text + \"} \" } return text } } Lets create some nodes and create a family tree structure let starks = Node ( value : \"Rickard Stark\" ) let s1 = Node ( value : \"Eddard Stark\" ) let s2 = Node ( value : \"Brandon Stark\" ) let s3 = Node ( value : \"Benjen Stark\" ) let c1 = Node ( value : \"Robb Stark\" ) let c2 = Node ( value : \"Sansa Stark\" ) let c3 = Node ( value : \"Arya Stark\" ) let c4 = Node ( value : \"Brandon Stark\" ) let c5 = Node ( value : \"Rickon Stark\" ) s1 . children = [ c1 , c2 , c3 , c4 , c5 ] starks . addChild ( child : s1 ) starks . addChild ( child : s2 ) starks . addChild ( child : s3 ) Lets print the entire family tree for Rickard Stark print ( starks ) Rickard Stark Children = { Eddard Stark Children = { Robb Stark , Sansa Stark , Arya Stark , Brandon Stark , Rickon Stark ,} , Brandon Stark , Benjen Stark ,} Lets print the entire family tree for Eddard Stark print ( s1 ) Eddard Stark Children = { Robb Stark , Sansa Stark , Arya Stark , Brandon Stark , Rickon Stark ,} Below we are checking if Arya Stark exists in the family tree of starks if let aryaStark = starks . search ( value : \"Arya Stark\" ) { print ( \"Found \\( aryaStark ) \" ) } else { print ( \"Not able to find Arya Stark\" ) } /* Found Arya Stark */ Lets check if John Snow belongs to the Stark family if let johnsnow = starks . search ( value : \"John Snow\" ) { print ( \"Found \\( johnsnow ) \" ) } else { print ( \"Not able to find John Snow\" ) } //Not able to find John Snow","title":"Tree"},{"location":"Datastructures/Tree/#tree-datastructure-implementation-in-swift","text":"import Cocoa class Node { var value : String = \"\" var children : [ Node ] = [] weak var parent : Node ? init ( value : String ) { self . value = value } func addChild ( child : Node ){ self . children . append ( child ) child . parent = self } func search ( value : String ) -> Node ?{ if value == self . value { return self } for child in self . children { if let foundObj = child . search ( value : value ){ return foundObj } } return nil } } extension Node : CustomStringConvertible { var description : String { var text = \" \\( value ) \" if ! children . isEmpty { text = text + \" \\n\\t Children = {\" for child in children { text = text + child . description + \",\" } text = text + \"} \" } return text } } Lets create some nodes and create a family tree structure let starks = Node ( value : \"Rickard Stark\" ) let s1 = Node ( value : \"Eddard Stark\" ) let s2 = Node ( value : \"Brandon Stark\" ) let s3 = Node ( value : \"Benjen Stark\" ) let c1 = Node ( value : \"Robb Stark\" ) let c2 = Node ( value : \"Sansa Stark\" ) let c3 = Node ( value : \"Arya Stark\" ) let c4 = Node ( value : \"Brandon Stark\" ) let c5 = Node ( value : \"Rickon Stark\" ) s1 . children = [ c1 , c2 , c3 , c4 , c5 ] starks . addChild ( child : s1 ) starks . addChild ( child : s2 ) starks . addChild ( child : s3 ) Lets print the entire family tree for Rickard Stark print ( starks ) Rickard Stark Children = { Eddard Stark Children = { Robb Stark , Sansa Stark , Arya Stark , Brandon Stark , Rickon Stark ,} , Brandon Stark , Benjen Stark ,} Lets print the entire family tree for Eddard Stark print ( s1 ) Eddard Stark Children = { Robb Stark , Sansa Stark , Arya Stark , Brandon Stark , Rickon Stark ,} Below we are checking if Arya Stark exists in the family tree of starks if let aryaStark = starks . search ( value : \"Arya Stark\" ) { print ( \"Found \\( aryaStark ) \" ) } else { print ( \"Not able to find Arya Stark\" ) } /* Found Arya Stark */ Lets check if John Snow belongs to the Stark family if let johnsnow = starks . search ( value : \"John Snow\" ) { print ( \"Found \\( johnsnow ) \" ) } else { print ( \"Not able to find John Snow\" ) } //Not able to find John Snow","title":"Tree Datastructure implementation in Swift"},{"location":"MachineLearning/DBScan_Clustering/DBScan_Clustering/","text":"DBScan Clustering Sample Created By: Debasis Das (26-Aug-2021) In this post we will generate a DBScan cluster using SKLearn DBSCAN module and will generate the following List of noise/outlier points (not readily available in DBSCAN model output) Index of noise/outlier points View the clusters by cluster label and identify all the core points in the clusters All the core points, count of core point, indexes of the core points Estimated number of clusters Silhouette Score of the Clustering Silhouette Score of the Clustering by ignoring the noise points Note: the noise points are those with label -1 import numpy as np from sklearn.cluster import DBSCAN from sklearn.metrics import silhouette_samples , silhouette_score import matplotlib.pyplot as plt epsilon = 3 min_samples = 2 X = [ [ 1 , 2 ], [ 2 , 2 ], [ 2 , 3 ], [ 8 , 7 ], [ 8 , 8 ], [ 25 , 80 ]] # plot A = np . array ( X ) plt . scatter ( A [:, 0 ], A [:, 1 ], c = 'white' , marker = 'o' , edgecolor = 'red' , s = 50 ) print ( \"The points prior to running DBScan Clustering\" ) plt . show () db = DBSCAN ( eps = epsilon , min_samples = min_samples ) . fit ( X ) core_samples_mask = np . zeros_like ( db . labels_ , dtype = bool ) core_samples_mask [ db . core_sample_indices_ ] = True labels = db . labels_ no_clusters = len ( np . unique ( labels ) ) no_noise = np . sum ( np . array ( labels ) == - 1 , axis = 0 ) noise_index = [] for i in range ( len ( labels )): if labels [ i ] == - 1 : noise_index . append ( i ) print ( \"Noise/Outliers Index are at the following indexes \" , noise_index ) for obj in noise_index : print ( \"Noise/outlier point = \" , X [ obj ]) print ( \"-\" * 20 ) unique_labels = set ( labels ) print ( \"unique labels are \" , unique_labels ) print ( \"-\" * 20 ) for label in unique_labels : print ( \"cluster points for \" , label ) cluster_indexes = [] for i in range ( len ( labels )): if labels [ i ] == label : cluster_indexes . append ( i ) print ( \"cluster_indexes = \" , cluster_indexes ) for obj in cluster_indexes : print ( \"exact point = \" , X [ obj ]) print ( \"-\" * 10 ) silhouette_avg = silhouette_score ( X , labels ) print ( \"-\" * 20 ) print ( \"Labels \" , labels ) print ( \"Indices of Core Samples = \" , db . core_sample_indices_ ) print ( \"Copy of each core sample found by training = \" , db . components_ ) print ( \"Count of Core Samples \" , len ( db . core_sample_indices_ )) print ( 'Estimated no. of clusters: %d ' % no_clusters ) print ( 'Estimated no. of noise points: %d ' % no_noise ) print ( \"-\" * 20 ) print ( \"silhouette_avg = \" , silhouette_avg ) print ( \"Silhoutter Avg ignoring noise points \" ,( silhouette_avg * len ( labels )) / ( len ( labels ) - no_noise ) ) The points prior to running DBScan Clustering Noise/Outliers Index are at the following indexes [5] Noise/outlier point = [25, 80] -------------------- unique labels are {0, 1, -1} -------------------- cluster points for 0 cluster_indexes = [0, 1, 2] exact point = [1, 2] exact point = [2, 2] exact point = [2, 3] ---------- cluster points for 1 cluster_indexes = [3, 4] exact point = [8, 7] exact point = [8, 8] ---------- cluster points for -1 cluster_indexes = [5] exact point = [25, 80] ---------- -------------------- Labels [ 0 0 0 1 1 -1] Indices of Core Samples = [0 1 2 3 4] Copy of each core sample found by training = [[1 2] [2 2] [2 3] [8 7] [8 8]] Count of Core Samples 5 Estimated no. of clusters: 3 Estimated no. of noise points: 1 -------------------- silhouette_avg = 0.7227526416896678 Silhoutter Avg ignoring noise points 0.8673031700276013","title":"DBScan Clustering(SKLearn)"},{"location":"MachineLearning/DBScan_Clustering/DBScan_Clustering/#dbscan-clustering-sample","text":"Created By: Debasis Das (26-Aug-2021) In this post we will generate a DBScan cluster using SKLearn DBSCAN module and will generate the following List of noise/outlier points (not readily available in DBSCAN model output) Index of noise/outlier points View the clusters by cluster label and identify all the core points in the clusters All the core points, count of core point, indexes of the core points Estimated number of clusters Silhouette Score of the Clustering Silhouette Score of the Clustering by ignoring the noise points Note: the noise points are those with label -1 import numpy as np from sklearn.cluster import DBSCAN from sklearn.metrics import silhouette_samples , silhouette_score import matplotlib.pyplot as plt epsilon = 3 min_samples = 2 X = [ [ 1 , 2 ], [ 2 , 2 ], [ 2 , 3 ], [ 8 , 7 ], [ 8 , 8 ], [ 25 , 80 ]] # plot A = np . array ( X ) plt . scatter ( A [:, 0 ], A [:, 1 ], c = 'white' , marker = 'o' , edgecolor = 'red' , s = 50 ) print ( \"The points prior to running DBScan Clustering\" ) plt . show () db = DBSCAN ( eps = epsilon , min_samples = min_samples ) . fit ( X ) core_samples_mask = np . zeros_like ( db . labels_ , dtype = bool ) core_samples_mask [ db . core_sample_indices_ ] = True labels = db . labels_ no_clusters = len ( np . unique ( labels ) ) no_noise = np . sum ( np . array ( labels ) == - 1 , axis = 0 ) noise_index = [] for i in range ( len ( labels )): if labels [ i ] == - 1 : noise_index . append ( i ) print ( \"Noise/Outliers Index are at the following indexes \" , noise_index ) for obj in noise_index : print ( \"Noise/outlier point = \" , X [ obj ]) print ( \"-\" * 20 ) unique_labels = set ( labels ) print ( \"unique labels are \" , unique_labels ) print ( \"-\" * 20 ) for label in unique_labels : print ( \"cluster points for \" , label ) cluster_indexes = [] for i in range ( len ( labels )): if labels [ i ] == label : cluster_indexes . append ( i ) print ( \"cluster_indexes = \" , cluster_indexes ) for obj in cluster_indexes : print ( \"exact point = \" , X [ obj ]) print ( \"-\" * 10 ) silhouette_avg = silhouette_score ( X , labels ) print ( \"-\" * 20 ) print ( \"Labels \" , labels ) print ( \"Indices of Core Samples = \" , db . core_sample_indices_ ) print ( \"Copy of each core sample found by training = \" , db . components_ ) print ( \"Count of Core Samples \" , len ( db . core_sample_indices_ )) print ( 'Estimated no. of clusters: %d ' % no_clusters ) print ( 'Estimated no. of noise points: %d ' % no_noise ) print ( \"-\" * 20 ) print ( \"silhouette_avg = \" , silhouette_avg ) print ( \"Silhoutter Avg ignoring noise points \" ,( silhouette_avg * len ( labels )) / ( len ( labels ) - no_noise ) ) The points prior to running DBScan Clustering Noise/Outliers Index are at the following indexes [5] Noise/outlier point = [25, 80] -------------------- unique labels are {0, 1, -1} -------------------- cluster points for 0 cluster_indexes = [0, 1, 2] exact point = [1, 2] exact point = [2, 2] exact point = [2, 3] ---------- cluster points for 1 cluster_indexes = [3, 4] exact point = [8, 7] exact point = [8, 8] ---------- cluster points for -1 cluster_indexes = [5] exact point = [25, 80] ---------- -------------------- Labels [ 0 0 0 1 1 -1] Indices of Core Samples = [0 1 2 3 4] Copy of each core sample found by training = [[1 2] [2 2] [2 3] [8 7] [8 8]] Count of Core Samples 5 Estimated no. of clusters: 3 Estimated no. of noise points: 1 -------------------- silhouette_avg = 0.7227526416896678 Silhoutter Avg ignoring noise points 0.8673031700276013","title":"DBScan Clustering Sample"},{"location":"MachineLearning/KMeans_Clustering/KMeans_Clustering/","text":"KMeans Clustering using SKLearn Created By: Debasis Das (26-Aug-2021) Using SKLearn to explore KMeans Clustering and Plotting the cluster centroids along with the cluster points import pandas as pd import numpy as np from sklearn.cluster import KMeans from sklearn.decomposition import PCA import matplotlib.pyplot as plt A = [ [ 10 , 10 ], [ 11 , 10 ], [ 12 , 10 ], [ 10 , 11 ], [ 15 , 17 ], [ 13.5 , 15 ], [ 15 , 16 ], [ 17 , 18 ], [ 14.5 , 19 ], [ 20 , 20 ], [ 13.5 , 14.5 ] ] df = pd . DataFrame ( A ) number_clusters = 3 pca = PCA ( n_components = 2 ) pca . fit ( df ) x_pca = pca . fit_transform ( df ) km = KMeans ( n_clusters = number_clusters , init = 'random' , n_init = 10 , max_iter = 300 , tol = 1e-04 , random_state = 0 ) . fit ( x_pca ) kmeans_labels = km . labels_ unique_labels = np . unique ( kmeans_labels ) for i in unique_labels : plt . scatter ( x_pca [ kmeans_labels == i , 0 ], x_pca [ kmeans_labels == i , 1 ] ) cluster_centroids = km . cluster_centers_ print ( \"cluster_centroids = \" , cluster_centroids ) print ( \"kmeans_labels = \" , kmeans_labels ) plt . scatter ( cluster_centroids [:, 0 ], cluster_centroids [:, 1 ], linewidths = 3 , s = 150 , marker = \"x\" , color = 'r' ) cluster_centroids = [[ 6.38423083 -1.0142913 ] [ 1.67176795 0.63576739] [-5.28182536 -0.28756359]] kmeans_labels = [2 2 2 2 1 1 1 0 1 0 1] <matplotlib.collections.PathCollection at 0x7fef5a604c40>","title":"KMeans Clustering(SKLearn)"},{"location":"MachineLearning/KMeans_Clustering/KMeans_Clustering/#kmeans-clustering-using-sklearn","text":"Created By: Debasis Das (26-Aug-2021) Using SKLearn to explore KMeans Clustering and Plotting the cluster centroids along with the cluster points import pandas as pd import numpy as np from sklearn.cluster import KMeans from sklearn.decomposition import PCA import matplotlib.pyplot as plt A = [ [ 10 , 10 ], [ 11 , 10 ], [ 12 , 10 ], [ 10 , 11 ], [ 15 , 17 ], [ 13.5 , 15 ], [ 15 , 16 ], [ 17 , 18 ], [ 14.5 , 19 ], [ 20 , 20 ], [ 13.5 , 14.5 ] ] df = pd . DataFrame ( A ) number_clusters = 3 pca = PCA ( n_components = 2 ) pca . fit ( df ) x_pca = pca . fit_transform ( df ) km = KMeans ( n_clusters = number_clusters , init = 'random' , n_init = 10 , max_iter = 300 , tol = 1e-04 , random_state = 0 ) . fit ( x_pca ) kmeans_labels = km . labels_ unique_labels = np . unique ( kmeans_labels ) for i in unique_labels : plt . scatter ( x_pca [ kmeans_labels == i , 0 ], x_pca [ kmeans_labels == i , 1 ] ) cluster_centroids = km . cluster_centers_ print ( \"cluster_centroids = \" , cluster_centroids ) print ( \"kmeans_labels = \" , kmeans_labels ) plt . scatter ( cluster_centroids [:, 0 ], cluster_centroids [:, 1 ], linewidths = 3 , s = 150 , marker = \"x\" , color = 'r' ) cluster_centroids = [[ 6.38423083 -1.0142913 ] [ 1.67176795 0.63576739] [-5.28182536 -0.28756359]] kmeans_labels = [2 2 2 2 1 1 1 0 1 0 1] <matplotlib.collections.PathCollection at 0x7fef5a604c40>","title":"KMeans Clustering using SKLearn"},{"location":"MachineLearning/MaximumLikelihood/MaximumLikelihood/","text":"Maximum Likelihood for Normal Distribution Added By (Debasis Das - 27-Aug-2021) Notes taken from StatQuest: Maximum Likelihood, clearly explained!!! Maximum Likelihood For the Normal Distribution, step-by-step!","title":"Maximum Likelihood"},{"location":"MachineLearning/MaximumLikelihood/MaximumLikelihood/#maximum-likelihood-for-normal-distribution","text":"Added By (Debasis Das - 27-Aug-2021) Notes taken from StatQuest: Maximum Likelihood, clearly explained!!! Maximum Likelihood For the Normal Distribution, step-by-step!","title":"Maximum Likelihood for Normal Distribution"},{"location":"Notes/OS_Notes/","text":"Introduction to Operating System Interrupt Steps Interrupts helps in doing context switching by tracking time Loading program counter and status register of interrupt vector Executing Interrupt Service Routines Pushing program counter and status register to stack Controlling Program Behavior Let a program run for a limited time and then switch to others Do not let programs write or read every part of the memory mechanism that helps with improving efficiency of resident monitor Buffering Spooling Caching advantage of privilege mode over user mode? It can execute system calls. It can execute any instruction. It can read/write any location in memory. time tracking happen in operating system? * Using CPU clock frequency approaches for memory protection? Allow programs to only write in their own specific memory part Do not allow programs to write over resident monitor role of memory management unit Checks programs request for writing into memory falls between base and base + bound Memory protection needed for CPU protection? To ensure programs can not execute register 0 (R0) CPU protection is fulfilled through Using hardware enforcement for user and privilege modes Making sure switching from user to privilege mode is supervised by resident monitor CPU Scheduling Concepts Process Process are active, Act of executing a program Requires memory to be assigned to it Programs Programs are passive One program can be run by multiple processes One process can run multiple programs A program can generate processes Program needs memory to be allocated for Code Global Data Stack Heap Programs are written in high level language Compiler Converts a program written in high level language into Machine readable language/ Assembly language Relative Reference Tag that corresponds to the address of a line of code Symbolic Reference Are tags for Library Functions Linker uses a relocation tables Uses a combination of object files and relocation table to identify loc that needs modification. Converts to binary Linker uses memory addresses starting from 0 and incremented by 1. This address is called relative addresses. Output of the linker is the .exe file Loader Symbol table contains relative addresses Loader loads the exe file, invokes the memory management unit, assigns physical memory addresses to the relative addresses Process states Initial READY- The process is waiting to be assigned to a processor. RUN- Instructions are being executed. WAIT- The process is waiting for some event to occur(such as an I/O completion or reception of a signal). Process from the wait state can only move to the ready state. EXIT - The process has finished execution. Free up the unused memory, Clean up the stack. Process Memory State Code Global Data Stack (function frames (Local variables, stack pointer, instruction pointer)) Heap (used for dynamic memory allocation) Stack and Heap needs dynamic memory allocation. Stack pointer and heap pointer are used to demarcate the start of stack and heap in memory Program Counter Stores the address of the next instruction to be executed Program counter are stored in EIP (Instruction Pointer) Status Register used to change mode (user vs privilege) MMU (Memory Management Unit) Used to do memory protection by utilizing a base and bound Process Control Block Holds the memory state of a process One PCB per process Contains registers, Program Counters, Stack Pointer, Heap Pointer, Process attributes, base , bound etc PCB is stored in the Kernel Data Structure stored in Kernel Memory space. Process ID, process priority Helps for switching between 2 processes. Scheduling Queues There are queues for the process states (Initial, Ready, Run, Wait and Exit) Run Queue can only hold one process, every other queue can hold more than one process Bursts Process contains a CPU and IO Bursts Process starts with a CPU Burst and should end with a CPU Burst Preemptive Scheduling CPU scheduler can stop processes in middle of running Processes which gets pre-empted goes to the ready state Non Premptive Scheduling Is used in embedded systems Cannot stop a process Context Switch Save context of the current running process in its process control block (PCB) Load context of the next running process from its process control block (PCB) SJF scheduling Process with shortest CPU burst time runs first Concurrent Processes Race Condition Read-Write Write- Write hardware solution for concurrency problem between threads Atomicity Atomic Instruction CPU will not context switch while executing this instruction progress property A thread executing non critical section code, should not block a thread willing to execute critical section CPU fairness assumption? In long term, threads should receive same number of CPU cycles software solutions for concurrency problem between threads such as Lamport-Bakery algorithm, can not be used in practice? System does not know number of threads and there is no way to determine it Readers-Writers problem Multiple readers can read at the same time Only one writer can write at a time If writer is writing, readers can not read If readers are reading, no writer can write Memory Management Internal Fragmentation Unused memory in the stack and heap allocated space External Fragmentation Unused memory between process allocated space Policies that can be used to improve efficiency of memory compaction Memory Allocation Protocols Dynamic Linking and Loading Overlays Swapping advantage of contiguous allocation over paging? For accessing lines of code, it requires less memory access page faults occur when A program wants to start executing Accessing a page that was previously in memory, but has been replaced by another page disadvantages of paging Increased memory usage, due to saving page tables Slow creation, due to logical to physical address conversion demand paging Virtual memory is based on using demand paging Pages will be moved to memory when they are accessed, and not before that. Initially there will be no program pages in memory, and they will all be kept in hard disk. Program Locality A page that is being used currently, has high chance of being used again. Two successive page requests have good chance of asking for the same page. Very few pages will have high frequency usage","title":"Operating System"},{"location":"Notes/OS_Notes/#introduction-to-operating-system","text":"Interrupt Steps Interrupts helps in doing context switching by tracking time Loading program counter and status register of interrupt vector Executing Interrupt Service Routines Pushing program counter and status register to stack Controlling Program Behavior Let a program run for a limited time and then switch to others Do not let programs write or read every part of the memory mechanism that helps with improving efficiency of resident monitor Buffering Spooling Caching advantage of privilege mode over user mode? It can execute system calls. It can execute any instruction. It can read/write any location in memory. time tracking happen in operating system? * Using CPU clock frequency approaches for memory protection? Allow programs to only write in their own specific memory part Do not allow programs to write over resident monitor role of memory management unit Checks programs request for writing into memory falls between base and base + bound Memory protection needed for CPU protection? To ensure programs can not execute register 0 (R0) CPU protection is fulfilled through Using hardware enforcement for user and privilege modes Making sure switching from user to privilege mode is supervised by resident monitor","title":"Introduction to Operating System"},{"location":"Notes/OS_Notes/#cpu-scheduling-concepts","text":"Process Process are active, Act of executing a program Requires memory to be assigned to it Programs Programs are passive One program can be run by multiple processes One process can run multiple programs A program can generate processes Program needs memory to be allocated for Code Global Data Stack Heap Programs are written in high level language Compiler Converts a program written in high level language into Machine readable language/ Assembly language Relative Reference Tag that corresponds to the address of a line of code Symbolic Reference Are tags for Library Functions Linker uses a relocation tables Uses a combination of object files and relocation table to identify loc that needs modification. Converts to binary Linker uses memory addresses starting from 0 and incremented by 1. This address is called relative addresses. Output of the linker is the .exe file Loader Symbol table contains relative addresses Loader loads the exe file, invokes the memory management unit, assigns physical memory addresses to the relative addresses Process states Initial READY- The process is waiting to be assigned to a processor. RUN- Instructions are being executed. WAIT- The process is waiting for some event to occur(such as an I/O completion or reception of a signal). Process from the wait state can only move to the ready state. EXIT - The process has finished execution. Free up the unused memory, Clean up the stack. Process Memory State Code Global Data Stack (function frames (Local variables, stack pointer, instruction pointer)) Heap (used for dynamic memory allocation) Stack and Heap needs dynamic memory allocation. Stack pointer and heap pointer are used to demarcate the start of stack and heap in memory Program Counter Stores the address of the next instruction to be executed Program counter are stored in EIP (Instruction Pointer) Status Register used to change mode (user vs privilege) MMU (Memory Management Unit) Used to do memory protection by utilizing a base and bound Process Control Block Holds the memory state of a process One PCB per process Contains registers, Program Counters, Stack Pointer, Heap Pointer, Process attributes, base , bound etc PCB is stored in the Kernel Data Structure stored in Kernel Memory space. Process ID, process priority Helps for switching between 2 processes. Scheduling Queues There are queues for the process states (Initial, Ready, Run, Wait and Exit) Run Queue can only hold one process, every other queue can hold more than one process Bursts Process contains a CPU and IO Bursts Process starts with a CPU Burst and should end with a CPU Burst Preemptive Scheduling CPU scheduler can stop processes in middle of running Processes which gets pre-empted goes to the ready state Non Premptive Scheduling Is used in embedded systems Cannot stop a process Context Switch Save context of the current running process in its process control block (PCB) Load context of the next running process from its process control block (PCB) SJF scheduling Process with shortest CPU burst time runs first","title":"CPU Scheduling Concepts"},{"location":"Notes/OS_Notes/#concurrent-processes","text":"Race Condition Read-Write Write- Write hardware solution for concurrency problem between threads Atomicity Atomic Instruction CPU will not context switch while executing this instruction progress property A thread executing non critical section code, should not block a thread willing to execute critical section CPU fairness assumption? In long term, threads should receive same number of CPU cycles software solutions for concurrency problem between threads such as Lamport-Bakery algorithm, can not be used in practice? System does not know number of threads and there is no way to determine it Readers-Writers problem Multiple readers can read at the same time Only one writer can write at a time If writer is writing, readers can not read If readers are reading, no writer can write","title":"Concurrent Processes"},{"location":"Notes/OS_Notes/#memory-management","text":"Internal Fragmentation Unused memory in the stack and heap allocated space External Fragmentation Unused memory between process allocated space Policies that can be used to improve efficiency of memory compaction Memory Allocation Protocols Dynamic Linking and Loading Overlays Swapping advantage of contiguous allocation over paging? For accessing lines of code, it requires less memory access page faults occur when A program wants to start executing Accessing a page that was previously in memory, but has been replaced by another page disadvantages of paging Increased memory usage, due to saving page tables Slow creation, due to logical to physical address conversion demand paging Virtual memory is based on using demand paging Pages will be moved to memory when they are accessed, and not before that. Initially there will be no program pages in memory, and they will all be kept in hard disk. Program Locality A page that is being used currently, has high chance of being used again. Two successive page requests have good chance of asking for the same page. Very few pages will have high frequency usage","title":"Memory Management"}]}